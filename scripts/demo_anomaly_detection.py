"""ç•°å¸¸æ¤œçŸ¥ãƒ‡ãƒ¢ã‚¹ã‚¯ãƒªãƒ—ãƒˆ.

æ··å…¥ã•ã›ãŸã€Œä¸å‚™ãƒ‡ãƒ¼ã‚¿ã€ã‚’AIãŒæ¤œçŸ¥ã—ã€ã‚¢ãƒ©ãƒ¼ãƒˆã‚’å‡ºã™ãƒ‡ãƒ¢ã€‚
"""

from __future__ import annotations

import json
import sys
from pathlib import Path

import numpy as np
import pandas as pd

sys.path.insert(0, str(__file__).replace("scripts/demo_anomaly_detection.py", "backend/src"))

from cs_risk_agent.analysis.benford import BenfordAnalyzer
from cs_risk_agent.analysis.rule_engine import RuleEngine


def load_demo_data() -> pd.DataFrame:
    """ãƒ‡ãƒ¢ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿."""
    data_path = Path(__file__).parent.parent / "demo_data" / "journal_entries.csv"
    if not data_path.exists():
        print("ãƒ‡ãƒ¢ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚å…ˆã« generate_demo_data.py ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
        print("  python scripts/generate_demo_data.py")
        sys.exit(1)
    return pd.read_csv(data_path)


def detect_statistical_anomalies(df: pd.DataFrame) -> list[dict]:
    """çµ±è¨ˆçš„ç•°å¸¸æ¤œçŸ¥."""
    anomalies = []

    # é‡‘é¡ã®ç•°å¸¸å€¤æ¤œçŸ¥ (Z-score > 3)
    amounts = df["debit"].replace(0, np.nan).dropna()
    if len(amounts) > 10:
        z_scores = (amounts - amounts.mean()) / amounts.std()
        outliers = df.loc[z_scores.abs() > 3]
        for _, row in outliers.iterrows():
            anomalies.append({
                "type": "statistical_outlier",
                "severity": "high",
                "entry_id": row["id"],
                "company_id": row["company_id"],
                "amount": row["debit"],
                "z_score": round(float(z_scores.loc[row.name]), 2),
                "description": f"çµ±è¨ˆçš„ç•°å¸¸å€¤ (Z-score: {z_scores.loc[row.name]:.2f})",
            })

    return anomalies


def detect_duplicate_amounts(df: pd.DataFrame) -> list[dict]:
    """é‡è¤‡é‡‘é¡æ¤œçŸ¥."""
    anomalies = []

    for company_id in df["company_id"].unique()[:10]:
        company_df = df[df["company_id"] == company_id]
        debits = company_df[company_df["debit"] > 0]["debit"]

        # åŒä¸€é‡‘é¡ãŒ3å›ä»¥ä¸Š
        counts = debits.value_counts()
        suspicious = counts[counts >= 3]

        for amount, count in suspicious.head(5).items():
            anomalies.append({
                "type": "duplicate_amount",
                "severity": "medium",
                "company_id": company_id,
                "amount": float(amount),
                "count": int(count),
                "description": f"åŒä¸€é‡‘é¡ Â¥{amount:,.0f} ãŒ {count}å› å‡ºç¾",
            })

    return anomalies


def detect_round_numbers(df: pd.DataFrame) -> list[dict]:
    """ç«¯æ•°ãªã—å¤§å£å–å¼•æ¤œçŸ¥."""
    anomalies = []

    large_round = df[
        (df["debit"] >= 1_000_000) &
        (df["debit"] % 1_000_000 == 0)
    ]

    for _, row in large_round.head(20).iterrows():
        anomalies.append({
            "type": "round_number",
            "severity": "medium",
            "entry_id": row["id"],
            "company_id": row["company_id"],
            "amount": row["debit"],
            "description": f"ç«¯æ•°ãªã—å¤§å£ä»•è¨³ Â¥{row['debit']:,.0f}",
        })

    return anomalies


def detect_weekend_entries(df: pd.DataFrame) -> list[dict]:
    """ä¼‘æ—¥è¨ˆä¸Šä»•è¨³æ¤œçŸ¥."""
    anomalies = []
    df_copy = df.copy()
    df_copy["day_of_week"] = pd.to_datetime(df_copy["date"]).dt.dayofweek

    weekend = df_copy[df_copy["day_of_week"] >= 5]

    for _, row in weekend.head(20).iterrows():
        day_name = "åœŸæ›œæ—¥" if row["day_of_week"] == 5 else "æ—¥æ›œæ—¥"
        anomalies.append({
            "type": "weekend_entry",
            "severity": "low",
            "entry_id": row["id"],
            "company_id": row["company_id"],
            "date": row["date"],
            "description": f"{day_name}ã«è¨ˆä¸Šã•ã‚ŒãŸä»•è¨³",
        })

    return anomalies


def run_benford_analysis(df: pd.DataFrame) -> dict:
    """ãƒ™ãƒ³ãƒ•ã‚©ãƒ¼ãƒ‰æ³•å‰‡åˆ†æ."""
    analyzer = BenfordAnalyzer()
    amounts = df[df["debit"] > 0]["debit"]
    result = analyzer.first_digit_test(amounts)
    return {
        "conformity": result.conformity,
        "mad": round(result.mad, 6),
        "chi_square": round(result.chi_square, 2),
        "p_value": round(result.p_value, 4),
        "sample_size": result.sample_size,
    }


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ."""
    print("=" * 70)
    print("  ç•°å¸¸æ¤œçŸ¥ãƒ‡ãƒ¢")
    print("  ä¸å‚™ãƒ‡ãƒ¼ã‚¿ã®è‡ªå‹•æ¤œçŸ¥ã¨ã‚¢ãƒ©ãƒ¼ãƒˆç”Ÿæˆ")
    print("=" * 70)

    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    print("\nğŸ“‚ ãƒ‡ãƒ¢ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ä¸­...")
    df = load_demo_data()
    print(f"  èª­è¾¼ä»¶æ•°: {len(df):,}ä»¶")
    print(f"  ä¼æ¥­æ•°: {df['company_id'].nunique()}ç¤¾")

    # æ—¢çŸ¥ã®ä¸å‚™ãƒ‡ãƒ¼ã‚¿æ•°
    known_anomalies = df[df.get("is_anomaly", False) == True] if "is_anomaly" in df.columns else pd.DataFrame()
    print(f"  æ··å…¥æ¸ˆã¿ä¸å‚™ãƒ‡ãƒ¼ã‚¿: {len(known_anomalies)}ä»¶")

    # å„æ¤œçŸ¥æ‰‹æ³•å®Ÿè¡Œ
    print("\nğŸ” ç•°å¸¸æ¤œçŸ¥ã‚’å®Ÿè¡Œä¸­...")

    all_anomalies = []

    print("\n  [1/5] çµ±è¨ˆçš„ç•°å¸¸å€¤æ¤œçŸ¥ (Z-score > 3)...")
    stat_anomalies = detect_statistical_anomalies(df)
    all_anomalies.extend(stat_anomalies)
    print(f"    â†’ {len(stat_anomalies)}ä»¶ æ¤œå‡º")

    print("\n  [2/5] é‡è¤‡é‡‘é¡æ¤œçŸ¥...")
    dup_anomalies = detect_duplicate_amounts(df)
    all_anomalies.extend(dup_anomalies)
    print(f"    â†’ {len(dup_anomalies)}ä»¶ æ¤œå‡º")

    print("\n  [3/5] ç«¯æ•°ãªã—å¤§å£å–å¼•æ¤œçŸ¥...")
    round_anomalies = detect_round_numbers(df)
    all_anomalies.extend(round_anomalies)
    print(f"    â†’ {len(round_anomalies)}ä»¶ æ¤œå‡º")

    print("\n  [4/5] ä¼‘æ—¥è¨ˆä¸Šä»•è¨³æ¤œçŸ¥...")
    weekend_anomalies = detect_weekend_entries(df)
    all_anomalies.extend(weekend_anomalies)
    print(f"    â†’ {len(weekend_anomalies)}ä»¶ æ¤œå‡º")

    print("\n  [5/5] ãƒ™ãƒ³ãƒ•ã‚©ãƒ¼ãƒ‰æ³•å‰‡åˆ†æ...")
    benford_result = run_benford_analysis(df)
    print(f"    â†’ é©åˆæ€§: {benford_result['conformity']}")
    print(f"    â†’ MAD: {benford_result['mad']}")
    print(f"    â†’ ã‚«ã‚¤äºŒä¹—å€¤: {benford_result['chi_square']}")

    # ã‚¢ãƒ©ãƒ¼ãƒˆã‚µãƒãƒªãƒ¼
    print("\n" + "=" * 70)
    print("  ğŸš¨ ç•°å¸¸æ¤œçŸ¥ã‚¢ãƒ©ãƒ¼ãƒˆ ã‚µãƒãƒªãƒ¼")
    print("=" * 70)

    by_severity = {}
    for a in all_anomalies:
        sev = a["severity"]
        by_severity[sev] = by_severity.get(sev, 0) + 1

    print(f"\n  æ¤œå‡ºç·æ•°: {len(all_anomalies)}ä»¶")
    for sev in ["high", "medium", "low"]:
        icon = {"high": "ğŸ”´", "medium": "ğŸŸ¡", "low": "ğŸŸ¢"}.get(sev, "âšª")
        print(f"    {icon} {sev}: {by_severity.get(sev, 0)}ä»¶")

    by_type = {}
    for a in all_anomalies:
        t = a["type"]
        by_type[t] = by_type.get(t, 0) + 1

    print(f"\n  æ¤œçŸ¥ã‚¿ã‚¤ãƒ—åˆ¥:")
    for t, count in sorted(by_type.items(), key=lambda x: -x[1]):
        print(f"    - {t}: {count}ä»¶")

    # ä¸Šä½ã‚¢ãƒ©ãƒ¼ãƒˆè¡¨ç¤º
    print(f"\n  ğŸ“‹ é«˜ãƒªã‚¹ã‚¯ã‚¢ãƒ©ãƒ¼ãƒˆ (ä¸Šä½10ä»¶):")
    high_alerts = [a for a in all_anomalies if a["severity"] == "high"]
    for i, alert in enumerate(high_alerts[:10], 1):
        print(f"    {i}. [{alert['type']}] {alert['description']}")

    print("\n" + "=" * 70)
    print("  ç•°å¸¸æ¤œçŸ¥ãƒ‡ãƒ¢å®Œäº†")
    print("=" * 70)


if __name__ == "__main__":
    main()
